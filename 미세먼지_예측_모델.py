# -*- coding: utf-8 -*-
"""미세먼지 예측 모델.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/113a668lFdlaiOBFcbXBtKtpzst3Xw1Z4
"""

pip install pandas numpy scikit-learn matplotlib statsmodels

import pandas as pd
import numpy as np
import re

import os
import pandas as pd

def combine_csv_files_from_folder(folder_path):
    """
    주어진 폴더 경로에서 모든 CSV 파일을 읽어와 하나의 데이터프레임으로 합칩니다.
    """
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

    data_frames = []
    for csv_file in csv_files:
        file_path = os.path.join(folder_path, csv_file)
        df = pd.read_csv(file_path)
        data_frames.append(df)

    if data_frames:
        return pd.concat(data_frames, ignore_index=True)
    else:
        return pd.DataFrame() # 빈 데이터프레임 반환

# 미세먼지 10 데이터 폴더 경로를 지정해주세요.
pm10_folder_path = './10/' # 예시 경로입니다. 실제 폴더 경로로 변경하세요.
# 미세먼지 2.5 데이터 폴더 경로를 지정해주세요.
pm25_folder_path = './2.5/' # 예시 경로입니다. 실제 폴더 경로로 변경하세요.

# 미세먼지 10 데이터프레임 생성
pm10_df = combine_csv_files_from_folder(pm10_folder_path)
if not pm10_df.empty:
    print(f"\nCombined PM10 DataFrame head (from {pm10_folder_path}):")
    print(pm10_df.head())
    print(f"Total rows in combined PM10 DataFrame: {len(pm10_df)}")
else:
    print(f"No CSV files found in the PM10 directory: {pm10_folder_path}")

# 미세먼지 2.5 데이터프레임 생성
pm25_df = combine_csv_files_from_folder(pm25_folder_path)
if not pm25_df.empty:
    print(f"\nCombined PM2.5 DataFrame head (from {pm25_folder_path}):")
    print(pm25_df.head())
    print(f"Total rows in combined PM2.5 DataFrame: {len(pm25_df)}")
else:
    print(f"No CSV files found in the PM2.5 directory: {pm25_folder_path}")

pm25_df.head()

len(pm10_df)

def wide_to_long(df_wide: pd.DataFrame, value_name: str):
    """
    df_wide columns 예:
    date, 측정망, 측정소명, 1시..24시, 지점ID, 지점
    """
    df = df_wide.copy()

    # 시간 컬럼 자동 탐지
    hour_cols = [c for c in df.columns if re.match(r"^\d{1,2}시$", str(c))]
    id_cols = [c for c in df.columns if c not in hour_cols]

    # long 변환
    dfl = df.melt(
        id_vars=id_cols,
        value_vars=hour_cols,
        var_name="hour",
        value_name=value_name
    )

    # 결측 표기 처리 + 숫자화
    dfl[value_name] = dfl[value_name].replace("-", np.nan)
    dfl[value_name] = pd.to_numeric(dfl[value_name], errors="coerce")

    # date + hour -> datetime
    dfl["date"] = pd.to_datetime(dfl["date"])
    dfl["hour"] = dfl["hour"].str.replace("시", "", regex=False).astype(int)

    # 24시 처리(다음날 00시로 넘기기)
    is_24 = dfl["hour"].eq(24)
    dfl.loc[is_24, "hour"] = 0
    dfl.loc[is_24, "date"] = dfl.loc[is_24, "date"] + pd.Timedelta(days=1)

    dfl["datetime"] = dfl["date"] + pd.to_timedelta(dfl["hour"], unit="h")

    # 정렬
    key_cols = ["측정소명", "datetime"] if "측정소명" in dfl.columns else ["datetime"]
    dfl = dfl.sort_values(key_cols).reset_index(drop=True)
    return dfl

def clean_by_station(df_long: pd.DataFrame, value_col: str, station_col="측정소명"):
    out = []
    for st, g in df_long.groupby(station_col):
        x = g.sort_values("datetime").copy().set_index("datetime")

        # 중복 datetime이 있으면 평균
        x = x.groupby(level=0)[value_col].mean().to_frame()

        # 시간축 재구성(빠진 시간 포함)
        full_idx = pd.date_range(x.index.min(), x.index.max(), freq="H")
        x = x.reindex(full_idx)

        # 결측 처리(필요에 맞게 조절)
        x[value_col] = x[value_col].interpolate(method="time", limit=6)
        x[value_col] = x[value_col].ffill().bfill()

        # 이상치 클리핑(1%~99%)
        lo, hi = x[value_col].quantile([0.01, 0.99])
        x[value_col] = x[value_col].clip(lo, hi)

        x = x.reset_index(names="datetime")
        x[station_col] = st
        out.append(x)

    # station_col 외 다른 메타(측정망/지점ID 등)는 merge로 다시 붙이는 게 안전
    return pd.concat(out, ignore_index=True)

pm25_long = wide_to_long(pm25_df, "pm25")
pm10_long = wide_to_long(pm10_df, "pm10")

# (선택) 정제
pm25_clean = clean_by_station(pm25_long[["측정소명","datetime","pm25"]], "pm25")
pm10_clean = clean_by_station(pm10_long[["측정소명","datetime","pm10"]], "pm10")

# 메타 붙이기(측정망/지점ID/지점은 원본에서 가져오되 중복 제거)
# '측정소명'을 한 번만 포함하도록 meta_cols를 수정합니다.
meta_cols = [c for c in pm25_long.columns if c not in ["date","hour","datetime","pm25","측정소명"] and not re.match(r"^\d{1,2}시$", str(c))]
meta_pm25 = pm25_long[meta_cols + ["측정소명"]].drop_duplicates(subset=["측정소명"], keep="first")

df = pm25_clean.merge(pm10_clean, on=["측정소명","datetime"], how="inner")
df = df.merge(meta_pm25, on="측정소명", how="left")

df = df.sort_values(["측정소명","datetime"]).reset_index(drop=True)
print(df.head())

def add_features(df):
    d = df.sort_values(["측정소명","datetime"]).copy()
    d["hour"] = d["datetime"].dt.hour
    d["dow"] = d["datetime"].dt.dayofweek
    d["month"] = d["datetime"].dt.month

    for col in ["pm10","pm25"]:
        for lag in [1,2,3,24,48]:
            d[f"{col}_lag_{lag}"] = d.groupby("측정소명")[col].shift(lag)
        for win in [3,6,24]:
            s = d.groupby("측정소명")[col].shift(1)
            d[f"{col}_roll_mean_{win}"] = s.rolling(win).mean().reset_index(level=0, drop=True)
            d[f"{col}_roll_std_{win}"]  = s.rolling(win).std().reset_index(level=0, drop=True)

    return d

df_feat = add_features(df)

from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

H = 1
tmp = df_feat.sort_values(["측정소명","datetime"]).copy()
tmp["y_pm10"] = tmp.groupby("측정소명")["pm10"].shift(-H)
tmp["y_pm25"] = tmp.groupby("측정소명")["pm25"].shift(-H)
tmp = tmp.dropna().reset_index(drop=True)

# 입력/타깃 구성
cat_cols = ["측정소명", "측정망"]  # 지점/지점ID도 있으면 여기 넣어도 됨
drop_cols = ["datetime","pm10","pm25","y_pm10","y_pm25"]
num_cols = [c for c in tmp.columns if c not in drop_cols + cat_cols]

X = tmp[num_cols + cat_cols]
X = pd.get_dummies(X, columns=cat_cols, dummy_na=True)
Y = tmp[["y_pm10","y_pm25"]]

# 시간 기반 split(각 지점별 마지막 20% 테스트)
tmp["rank"] = tmp.groupby("측정소명")["datetime"].rank(pct=True)
train_idx = tmp["rank"] <= 0.8
test_idx  = tmp["rank"] > 0.8

X_train, X_test = X.loc[train_idx], X.loc[test_idx]
Y_train, Y_test = Y.loc[train_idx], Y.loc[test_idx]

base = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05, max_iter=400, random_state=42)
model = MultiOutputRegressor(base)
model.fit(X_train, Y_train)

pred = model.predict(X_test)

print("PM10 MAE/RMSE:",
      mean_absolute_error(Y_test["y_pm10"], pred[:,0]),
      mean_squared_error(Y_test["y_pm10"], pred[:,0])**0.5)

print("PM25 MAE/RMSE:",
      mean_absolute_error(Y_test["y_pm25"], pred[:,1]),
      mean_squared_error(Y_test["y_pm25"], pred[:,1])**0.5)